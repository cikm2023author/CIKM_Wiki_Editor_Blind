{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fn = \"data/new_pol/ampol_train.tsv\"\n",
    "test_fn = \"data/new_pol/ampol_test.tsv\"\n",
    "val_fn = \"data/new_pol/ampol_val.tsv\"\n",
    "vocab_fn = 'data/bert_vocab.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_fn, sep='\\t')\n",
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute user clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "userInsertions = defaultdict(int)\n",
    "userDeletions = defaultdict(int)\n",
    "userSkips = defaultdict(int)\n",
    "userReplaces = defaultdict(int)\n",
    "\n",
    "users = defaultdict(list)\n",
    "\n",
    "for ridx, row in df.iterrows():\n",
    "    user = row['user']\n",
    "    user = user.rstrip()\n",
    "    user = user.rstrip('\\\\')\n",
    "    users[user] = []\n",
    "    \n",
    "    edit_string = row['edit_string']\n",
    "    u_edits = edit_string.split(' ')\n",
    "    for u_edit in u_edits:\n",
    "        if u_edit == 'SKIP':\n",
    "            userSkips[user] += 1\n",
    "        elif u_edit == 'DELETE':\n",
    "            userDeletions[user] += 1\n",
    "        elif u_edit == 'INSERT':\n",
    "            userInsertions[user] += 1\n",
    "        elif u_edit == 'REPLACE':\n",
    "            userReplaces[user] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userTuples = {}\n",
    "for user in users:\n",
    "    totalNonSkip = userInsertions[user] + userDeletions[user] + userReplaces[user]\n",
    "    totalEdits = totalNonSkip + userSkips[user]\n",
    "\n",
    "    # What percent of interesting edits are delete/insert/replace\n",
    "    deleteRate = float(userDeletions[user]) / float(totalNonSkip)\n",
    "    insertRate = float(userInsertions[user]) / float(totalNonSkip)\n",
    "    replaceRate = float(userReplaces[user]) / float(totalNonSkip)\n",
    "\n",
    "    # What is the total rate of skips\n",
    "    skipRate = float(userSkips[user]) / float(totalEdits)\n",
    "    \n",
    "    userTuples[user] = (deleteRate, insertRate, replaceRate, skipRate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "userOrder = [user for user in userTuples]\n",
    "X = np.array([userTuples[user] for user in userOrder], np.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import Birch\n",
    "\n",
    "model = Birch(threshold=0.01, n_clusters=16)\n",
    "# fit the model\n",
    "model.fit(X)\n",
    "# assign a cluster to each example\n",
    "yhat = model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wiki_ucl = pd.DataFrame(list(zip(userOrder,yhat)),columns=['user','cluster'])\n",
    "for i in range(16):\n",
    "    print(wiki_ucl[wiki_ucl['cluster'] == i].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userClusters = {}\n",
    "for ridx,row in wiki_ucl.iterrows():\n",
    "    userClusters[row['user']] = row['cluster']\n",
    "userClusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xClusters = defaultdict(list)\n",
    "for idx in range(len(yhat)):\n",
    "    cluster = yhat[idx]\n",
    "    xClusters[cluster] += [X[idx]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cSums = {}\n",
    "for cluster in xClusters:\n",
    "    cList = xClusters[cluster]\n",
    "    cSum = np.zeros([1,4])\n",
    "    for item in cList:\n",
    "        cSum += item\n",
    "    cSums[cluster] = (cSum) / len(cList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cSums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute user tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(vocab_fn,'r') as vfiler:\n",
    "    vocaball = vfiler.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = []\n",
    "for iid, item in enumerate(vocaball):\n",
    "    item = item.strip()\n",
    "    if not item.isalpha():\n",
    "        continue\n",
    "    try:\n",
    "        item.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "    vocab += [item]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ridx, row in df.iterrows():\n",
    "    user = row['user']\n",
    "    user = user.rstrip()\n",
    "    user = user.rstrip('\\\\')\n",
    "    users[user] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give each user a random tag from the vocabulary\n",
    "for uidx, user in enumerate(users):\n",
    "    users[user] += [random.choice(vocab)]\n",
    "    users[user] += [random.choice(vocab)]\n",
    "    users[user] += ['user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to add line_id index to tsv files if they are absent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if no line ids in tsv_in, first add them. Also replace underscores in titles with spaces, and randomize rows\n",
    "tsv_file_in_no_lids = 'data/new_pol/ampol_train.tsv'\n",
    "tsv_out_w_lids = \"data/new_pol/ampol_train_wlids.tsv\"\n",
    "\n",
    "df = pd.read_csv(tsv_file_in_no_lids, sep='\\t')\n",
    "df = df.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'line_id' not in df.columns:\n",
    "    line_ids = range(df.shape[0])\n",
    "    df['line_id'] = line_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ridx, row in df.iterrows():\n",
    "    title = row['article_title']\n",
    "    title_words = title.split('_')\n",
    "    title = ' '.join(title_words)\n",
    "    df.at[ridx, 'article_title'] = title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.set_index('line_id')\n",
    "df.to_csv(tsv_out_w_lids, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to add annotator model outputs to tsv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_generated = \"data/new_pol/ampol_annotator_output_train.txt\" # annotator edit string outputs generated by t5\n",
    "line_id_file = 'data/new_pol/ampol_train_LIDS.txt'  # output by jsonl code below\n",
    "all_data_file = \"data/new_pol/ampol_train.tsv\"\n",
    "outfile = 'data/new_pol/ampol_annotator_temp_train.txt'\n",
    "final_outfile = 'data/new_pol/ampol_generator_input_train.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newlines = []\n",
    "\n",
    "with open(t5_generated, 'r') as t5f:\n",
    "    for lidx, line in enumerate(t5f.readlines()):\n",
    "        line = line.strip()\n",
    "        newlines += [line]\n",
    "    lcount = lidx\n",
    "    \n",
    "with open(line_id_file, 'r') as lidf:\n",
    "    for lidx, line in enumerate(lidf.readlines()):\n",
    "        line = line.strip()\n",
    "        newlines[lidx] = newlines[lidx] + '\\t' + line + '\\n'\n",
    "    rcount = lidx\n",
    "    \n",
    "assert lcount == rcount\n",
    "\n",
    "with open(outfile, 'w') as outf:\n",
    "    outf.write(\"edit_string_predicted\\tline_id\\n\")\n",
    "    for line in newlines:\n",
    "        outf.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edit_preds = pd.read_csv(outfile, sep='\\t')\n",
    "df_all_data = pd.read_csv(all_data_file, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_edit_preds = df_edit_preds.set_index('line_id')\n",
    "df_all_data = df_all_data.set_index('line_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = df_edit_preds.join(df_all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.to_csv(final_outfile, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Produce jsonlines file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Json files for t5\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "tsv_file_in = 'data/new_pol/ampol_train_wlids.tsv'\n",
    "jsonl_file_out = 'data/new_pol/ampol_train_generator.json'\n",
    "lid_file_out = 'data/new_pol/ampol_train_generator_LIDS.txt' # remember line ids in separate file\n",
    "\n",
    "# EDIT THESE BOOLEANS to append tags, etc\n",
    "# ---------------------------------------\n",
    "\n",
    "# ANNOTATE MODEL INPUT\n",
    "edit_outputs = False           # expects data field edit_string, computed by Levenshtein notebook\n",
    "edit_outputs_no_skip = False   # ignore skips\n",
    "\n",
    "# GENERATOR MODEL INPUT\n",
    "append_edit_string = False     # expects data field edit_string, computed by Levenshtein notebook (ground truth)\n",
    "append_edit_summary = False    # ignore skips\n",
    "append_pred_edit_string = True # expects data field edit_string_predicted, output from annotator model\n",
    "\n",
    "# PERSONALIZATION\n",
    "add_user_tags = True\n",
    "add_user_cluster = True\n",
    "\n",
    "# ---------------------------------------\n",
    "\n",
    "df = pd.read_csv(tsv_file_in, sep='\\t')\n",
    "df = df.fillna('')\n",
    "df = df.set_index('line_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num2words = {0: 'Zero', 1: 'One', 2: 'Two', 3: 'Three', 4: 'Four', 5: 'Five', \\\n",
    "             6: 'Six', 7: 'Seven', 8: 'Eight', 9: 'Nine', 10: 'Ten', \\\n",
    "            11: 'Eleven', 12: 'Twelve', 13: 'Thirteen', 14: 'Fourteen', \\\n",
    "            15: 'Fifteen'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "empty_refs_count = 0\n",
    "same_count=0\n",
    "user_not_count=0\n",
    "unknownCount = 0\n",
    "# Output in JSONL format\n",
    "with open(jsonl_file_out,'w') as outfile, open(lid_file_out,'w') as lidoutfile:\n",
    "    for ridx, row in df.iterrows():\n",
    "        user = row['user'].strip()\n",
    "        article_title = row['article_title']\n",
    "        in_text = row['parent text']\n",
    "        out_text = row['revision text']\n",
    "\n",
    "        if append_pred_edit_string:\n",
    "            in_text = in_text + ', metadata: ' + row['edit_string_predicted']\n",
    "        if append_edit_summary:\n",
    "            in_text = in_text + ', metadata: '\n",
    "            edit_items = row['edit_string'].split(' ')\n",
    "            for edit_item in edit_items:\n",
    "                if edit_item != 'SKIP':\n",
    "                    in_text = in_text + ' ' + edit_item\n",
    "        \n",
    "        line_id = ridx\n",
    "        \n",
    "        if in_text.strip() == '':\n",
    "            continue\n",
    "            \n",
    "        if out_text.strip() == '':\n",
    "            empty_refs_count += 1\n",
    "            continue\n",
    "            \n",
    "        if in_text.strip() == out_text.strip():\n",
    "            same_count += 1\n",
    "            continue\n",
    "        \n",
    "        if add_user_tags:\n",
    "            user_tags_str = ' '.join(users[user])\n",
    "            in_text = user_tags_str + ' ' + in_text\n",
    "            \n",
    "        if add_user_cluster:\n",
    "            cluster = userClusters[user]\n",
    "            in_text = in_text + ', metadata: user cluster ' + num2words[cluster]\n",
    "            \n",
    "        # instead of en_out as target string, en_out is edit string\n",
    "        if edit_outputs_no_skip:\n",
    "            out_text = ''\n",
    "            edit_items = row['edit_string'].split(' ')\n",
    "            for edit_item in edit_items:\n",
    "                if edit_item != 'SKIP':\n",
    "                    out_text = out_text + edit_item + ' '\n",
    "            out_text = out_text[:-1]\n",
    "            if out_text.strip() == '':\n",
    "                out_text = \"SKIP\"\n",
    "            \n",
    "        if edit_outputs:\n",
    "            out_text = ''\n",
    "            edit_items = row['edit_string'].split(' ')\n",
    "            for edit_item in edit_items:\n",
    "                out_text = out_text + edit_item + ' '\n",
    "            out_text = out_text[:-1]\n",
    "         \n",
    "        # format should be e.g.:\n",
    "        # \"translation\": { \"en_in\": \"this is prev text\", \"en_out\": \"this is post text\"} }\n",
    "        line = '{\"translation\": { \"en_in\": \"' + in_text + '\", \"en_out\": \"' + out_text + '\"} }\\n'\n",
    "        \n",
    "        if add_user_tags and 'user' not in in_text.split(' '):\n",
    "            user_not_count += 1\n",
    "            continue\n",
    "        \n",
    "        outfile.write(line)\n",
    "        lidoutfile.write(str(line_id) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
